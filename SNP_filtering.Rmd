---
title: "SNP filtering"
author: "Alice Feurtey"
date: "01/13/2021"
output:
  html_document: default
    code_folding: hide
---

```{r setup, include=TRUE}
#Libraries needed
library(tidyverse)
library(ggExtra)
library(tidygraph)
library(ggraph)
library(cowplot)
library(kableExtra)
library(gridExtra)

#Chromosome choice for different steps
test_CHR = 10
subset_CHR = 1

#Files and directories 
dir_name = "/data2/alice/WW_project/"
data_dir = "/data2/alice/WW_project/0_Data/"
test_vcf_name = paste0(dir_name, 
                  "1_Variant_calling/4_Joint_calling/",
                  "Ztritici_global_December2020.genotyped.", 
                  test_CHR)
subset_vcf_name = paste0(dir_name, 
                  "1_Variant_calling/4_Joint_calling/",
                  "Ztritici_global_December2020.genotyped.", 
                  subset_CHR,
                  ".filtered.clean")
ref_genome = paste0(dir_name, 
                    "0_Data/",
                    "Zymoseptoria_tritici.MG2.dna.toplevel.mt+.fa")
vcf_qual_check_dir =paste0(dir_name, 
                           "1_Variant_calling/5_Quality_checks/")

soft_dir = "/home/alice/Software/"
list_dir = "/home/alice/WW_PopGen/Keep_lists_samples/"
gatk_path = paste0(soft_dir, "gatk-4.1.4.1/gatk")
#java_path = "/Library/Java/JavaAppletPlugin.plugin/Contents/Home/bin/java"
bcftools_path = paste0(soft_dir, "bcftools-1.10.2/")

#This is a file that contains the name of all samples I am not interested in
# for example Michael Habig's mutants and other resequencing of IPO323.
unwanted_samples=read_tsv(paste0(data_dir, "Samples_to_filter_out.args"), col_names = F) %>% pull()

Sys.setenv(DIRNAME=dir_name)
Sys.setenv(TESTVCFNAME=test_vcf_name)
Sys.setenv(SUBVCFNAME=subset_vcf_name)
Sys.setenv(REF=ref_genome)
Sys.setenv(GATKPATH=gatk_path)
#Sys.setenv(JAVAPATH=java_path)
Sys.setenv(BCFTOOLSPATH=bcftools_path)

```


# Filtering based on site: identifying the proper values
Below I first extract the snps and in a second time, I remove all genotypes with a depth that I consider too low for the data to be reliable. This threshold can and should be adapted depending on the dataset.
Finally, I use bcftools to extract the values of the different estimates we can filter on.
```{bash, eval = F}

#Removing indels
${GATKPATH} SelectVariants  \
  -R ${REF} \
  -V ${TESTVCFNAME}.vcf.gz \
 --select-type-to-include SNP \
 -O ${TESTVCFNAME}.snps.vcf

#Filtering on low depth per genotype
${GATKPATH} VariantFiltration \
 -R ${REF} \
 -V ${TESTVCFNAME}.snps.vcf \
--genotype-filter-expression "DP < 3"  \
--set-filtered-genotype-to-no-call   \
--genotype-filter-name "Low_depth"   \
-O ${TESTVCFNAME}.snps.low_depth.vcf
```
Here, I clean the file to remove alternate alleles that do not exist anymore because they were only found in the filtered genotyped. And I additionally remove the positions which are non-variant, now that some genotypes are filtered out.

```{bash, eval = F}
${GATKPATH} SelectVariants  \
  -R ${REF} \
  -V ${TESTVCFNAME}.snps.low_depth.vcf \
 --exclude-non-variants --remove-unused-alternates \
 -O ${TESTVCFNAME}.snps.low_depth.variants.vcf
 
```

```{bash, eval = F}
#Extracting the data for the figures
${BCFTOOLSPATH}bcftools query -f '%CHROM %POS %MQ %QD %FS %DP %MQRankSum %QUAL %ReadPosRankSum %BaseQRankSum\n'  ${TESTVCFNAME}.snps.low_depth.variants.vcf > ${TESTVCFNAME}.snps.low_depth.variants.info_fields.tab

```

Now that we have the data extracted, I can make figures and test values for the filtering.

```{r}
FS_thr = 10
MQ_thr = 20
QD_thr = 20
DP_thr = 60000
ReadPosRankSum_thr_high = 2
ReadPosRankSum_thr_low = -2
MQRankSum_thr_high = 2
MQRankSum_thr_low = -2
BaseQRankSum_thr_high = 2
BaseQRankSum_thr_low = -2
  
print(paste0(test_vcf_name, ".snps.low_depth.variants.info_fields.tab"))
info = read_delim(paste0(test_vcf_name, ".snps.low_depth.variants.info_fields.tab"),
                  delim = " ", col_names =  c("CHROM", "POS", "MQ", "QD", "FS", "DP",
                                              "MQRankSum", "QUAL", "ReadPosRankSum", "BaseQRankSum"), na = ".")
temp = info %>% 
  gather(key = "info_field", value = "value", -CHROM, -POS) %>% 
  mutate (Filter = "Unfiltered")

temp2 = info  %>% filter(FS < FS_thr & MQ > MQ_thr & QD > QD_thr & DP < DP_thr) %>% 
  filter(between(MQRankSum, MQRankSum_thr_low, MQRankSum_thr_high) | is.na(MQRankSum)) %>% 
  filter(between(BaseQRankSum, BaseQRankSum_thr_low, BaseQRankSum_thr_high) | is.na(BaseQRankSum)) %>% 
  filter(between(ReadPosRankSum, ReadPosRankSum_thr_low, ReadPosRankSum_thr_high) | is.na(ReadPosRankSum)) %>% 
  filter(QUAL > 500) %>%
  gather(key = "info_field", value = "value", -CHROM, -POS) %>% 
  mutate (Filter = "Filtered")

p = ggplot(temp, aes(value, fill = Filter)) + geom_density(alpha=0.6) 
p + facet_wrap(info_field~., scales="free")  + theme_classic() 

p2 = ggplot(temp2, aes(value, fill = Filter)) + geom_density(alpha=0.6) 
p2 + facet_wrap(info_field~., scales="free")  + theme_classic() 


dim(temp %>% select(CHROM, POS) %>% group_by_all %>% count)
dim(temp2 %>% select(CHROM, POS) %>% group_by_all %>% count) 


Sys.setenv(FSTHR = sprintf("%0.2f", FS_thr))
Sys.setenv(MQTHR = sprintf("%0.2f", MQ_thr))
Sys.setenv(QDTHR = sprintf("%0.2f", QD_thr))
Sys.setenv(DPTHR = sprintf("%0.2f", DP_thr))
```

The values obtained above can be used in a script to run directly on the cluster for all chromosomes.



# Filter checks using resequencing

I want to assess the effect of the filtering on the SNP data. 
```{r Get variants from filter}

var_list = bind_rows(paste0(vcf_qual_check_dir, "Ztritici_global_December2020.genotyped.", c(1:21), ".tab") %>%
                         map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT"), 
                           col_types = cols(col_character(), col_double(), col_character(), col_character()))) %>%
                         mutate(Filter = "1_Raw_variants"),
                     paste0(vcf_qual_check_dir, "Ztritici_global_December2020.genotyped.", c(1:21), ".filtered.clean.tab") %>%
                         map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT"), 
                           col_types = cols(col_character(), col_double(), col_character(), col_character()))) %>%
                         mutate(Filter = "2_Filtered")) %>% 
  mutate(Nb_allele = str_count(ALT, pattern = ",") + 2) %>%
  mutate(Nb_letter = str_count(ALT, pattern = "[A-Z]") + str_count(REF, pattern = "[A-Z]")) %>%
  mutate(Indel = ifelse((str_count(ALT, pattern = "[*]") + str_count(REF, pattern = "[*]")) > 0, "Indel", 
                        ifelse(Nb_letter != Nb_allele, "Indel", "SNP"))) %>%
  unite(col = "Var_type", Indel, Nb_allele, sep = "_", remove = F)

var_list  %>%
group_by(CHROM, Filter, Var_type) %>%
count()  %>%
ggplot(aes(x=CHROM, y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "stack") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_wrap(vars(Filter)) +
  coord_flip() +
  labs(title = "Variants number before and after filtering on position quality")

```


We have 9 isolates that were resequenced several times in different datasets. Using these, we can estimate the amount of false positive that happen and the type of variants that are more susceptible to be affected by such errors.
```{r tab per pair}
reseq_list = read_delim(paste0(list_dir, "Repeat_sequencing.txt"), delim =" ", col_names = c("Name_1", "Name_2"))


#Read data for all chromosomes and for each pair of samples
list_diff = list()
for (sample in c(1:length(reseq_list$Name_1))) {

list_filtered = list.files(path = vcf_qual_check_dir, pattern = paste0("filtered.clean.", reseq_list$Name_1[sample], ".tab"), full.names = T) 
list_unfiltered = setdiff(list.files(path = vcf_qual_check_dir, pattern = paste0(reseq_list$Name_1[sample], ".tab"), full.names = T),
                     list.files(path = vcf_qual_check_dir, pattern = paste0("filtered.clean.", reseq_list$Name_1[sample], ".tab"), 
                                full.names = T)) 

filtered = list_filtered %>% 
    map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT", "Clone1", "Clone2"), 
                        col_types = cols(col_character(), col_double(), col_character(), col_character(), col_character(), col_character()))) %>%
    mutate(Filter = "2_Filtered") 



unfiltered = list_unfiltered %>% 
    map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT", "Clone1", "Clone2"), 
                        col_types = cols(col_character(), col_double(), col_character(), col_character(), col_character(), col_character()))) %>%
    mutate(Filter = "1_Raw_variants") 


head(unfiltered)
list_diff[[sample]] = bind_rows(filtered, unfiltered) %>% mutate(Sample = reseq_list$Name_1[sample]) 
}

# Gather every table previously read in one single dataframe
# and transform to estimate the number of allele and whether the position is only SNPs or has at least one indel
temp2 = bind_rows(list_diff) %>% 
  mutate(Nb_allele = str_count(ALT, pattern = ",") + 2) %>%
  mutate(Nb_letter = str_count(ALT, pattern = "[A-Z]") + str_count(REF, pattern = "[A-Z]")) %>%
  mutate(Indel = ifelse((str_count(ALT, pattern = "[*]") + str_count(REF, pattern = "[*]")) > 0, "Indel", 
                        ifelse(Nb_letter != Nb_allele, "Indel", "SNP"))) %>%
  unite(col = "Var_type", Indel, Nb_allele, sep = "_", remove = F)


t2 = as.data.frame(table(temp2 %>% 
  group_by(CHROM, POS, Filter, Var_type) %>% 
  count() %>%
    ungroup() %>%
  select(n)))

nb_shared = sum(t2[as.numeric(t2$Var1) > 1,]$Freq)
nb_all = sum(t2$Freq)
ggplot(t2, aes(x = Var1, y = Freq, label = Freq)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  geom_label(nudge_y = 5000) +
  labs(x = "Number of pairs sharing an erroneous locus",
       y = "Number of loci",
       title = "Some erroneous loci are shared between samples",
       subtitle = paste0("Total number of erroneous loci: ", nb_all, "; shared loci: ", nb_shared))
```
I not only want to know how many loci contain genotyping erros but also what type of variants these are. So I will make some more plots to figure this out.
```{r plot reseq pairs}
# Bar plot of variant type per resequencing pair
p1 = temp2  %>%
group_by(Filter, Sample, Var_type) %>%
count()  %>%
ggplot(aes(x=Sample, y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "stack") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_wrap(vars(Filter)) +
  coord_flip()

# Bar plot of variant type per chromosome
p2 = temp2  %>%
  select(CHROM, POS, Filter, Var_type) %>%
  distinct() %>%
group_by(CHROM, Filter, Var_type) %>%
count()  %>%
ggplot(aes(x=as.numeric(CHROM), y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "stack") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_grid(cols = vars(Filter)) 

legend_b <- get_legend(
  p1 + 
    guides(color = guide_legend(nrow = 1)) +
    theme(legend.position = "bottom")
)

prow = plot_grid( p1 + theme(legend.position="none", axis.title = element_blank()),
               p2 + theme(legend.position="none", axis.title = element_blank()),
               nrow = 1, rel_widths = c(1, 0.8))
plot_grid(prow, legend_b, rel_heights = c(1, 0.1), ncol = 1)
```


Let's visualize both datasets together so we can compare the proportions of all variants vs errors.
```{r}
#Bar plots of the comparison
p1 = bind_rows(temp2 %>%
            unite(col = "Var_type", Indel, Nb_allele, sep = "_")%>%
            select(CHROM, POS, Filter, Var_type) %>%
            distinct() %>% 
            mutate(Dataset = "Clones"),
          var_list%>% 
            mutate(Dataset = "All_variants")) %>%
  group_by(Filter, Var_type, Dataset) %>%
count()  %>%
ggplot(aes(x=Dataset, y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "fill") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_grid(cols = vars(Filter)) +
  labs(x = "", y = "Proportion of variants")

legend_b <- get_legend(
  p1 + 
    guides(color = guide_legend(nrow = 1)) +
    theme(legend.position = "right")
)


#Prep summary table
t1 = var_list %>% 
  group_by(Filter) %>%
  count()  
t2 = var_list %>% 
  filter(Indel == "SNP") %>%
  group_by(Filter) %>%
  count()   

temp = tibble(Filter_status = c("Filtered out", "Kept after filtering","Filtered out", "Kept after filtering"),
       Variant_type = c("All", "All", "SNP", "SNP"),
       Nb_variants = c((t1[2, 2] - t1[1, 2]) %>% pull(), t1[1, 2] %>% pull(),
                       (t2[2, 2] - t2[1, 2]) %>% pull(), t2[1, 2] %>% pull() )) %>%
  pivot_wider(names_from = Variant_type, values_from = Nb_variants)


# Plot everything together
col1 = plot_grid(tableGrob(temp , rows=NULL), legend_b, ncol = 1)
plot_grid(p1 + theme(legend.position = "none", plot.margin = margin(0.5, 1.5, 0.5, 0.5, "cm")), 
          col1, rel_widths = c(1, 0.7))

```

Once we have a general idea of the error quantity, I want to look at the variants distribution along the genomes. We already know that some erroneous variants are actually found in multiple resequencing pairs. But even more generally, are errors found in the same area of the genomes for all resequencing pairs? 
```{r diff per windows}

#Let's first create windows along the whole genome 
chromosomes = read_tsv(paste0(ref_genome, ".fai"),
                        col_names = c("CHROM", "Length", "Byte_index", "Base_per_line", "Bytes_per_line")) 
win_size = 10000

list_temp = list()
for (chr in c(1:length(chromosomes$CHROM))) {
  list_temp[[chr]] = bind_rows(tibble(CHROM = as.character(chr), 
                            Window = seq(from = 0, to = chromosomes$Length[14], by = win_size), 
                            Filter = "1_Raw_variants"),
                            tibble(CHROM = as.character(chr), 
                            Window = seq(from = 0, to = chromosomes$Length[14], by = win_size), 
                            Filter = "2_Filtered"))

}

windows = (bind_rows(list_temp))


# Transforming the tables with the differences to match the windows
temp = bind_rows(list_diff) %>% 
  mutate(Window = win_size*round(POS / win_size)) %>%
  group_by(CHROM, Filter, Sample, Window) %>%
  count() %>%
  pivot_wider(names_from = Sample, values_from = n) %>%
  full_join(., windows) %>%
  pivot_longer(-c(CHROM, Filter, Window), names_to = "Sample", values_to = "Nb_SNP") %>%
  mutate(Nb_SNP = replace_na(Nb_SNP, 0))

# And finally, plotting the results
temp %>%
  filter(Filter == "1_Raw_variants") %>%
  filter(as.integer(CHROM) <= 6) %>% 
ggplot(., aes(x = Window, y = Nb_SNP, col = Sample)) +
  geom_line(alpha = .4) +
  geom_point(alpha = .4) + 
  theme_minimal() + 
  facet_grid(rows = vars(as.integer(CHROM)), cols = vars(Filter))

temp %>%
  filter(as.integer(CHROM) <= 6) %>% 
ggplot(., aes(x = Window, y = Nb_SNP, col = Sample)) +
  geom_line(alpha = .4) +
  theme_minimal() + 
  facet_grid(rows = vars(as.integer(CHROM)), cols = vars(Filter)) +
  theme(legend.position = "none")

```




# Filtering based on individuals
```{bash, eval = F}
~/Software/vcftools_jydu/src/cpp/vcftools \
  --gzvcf ${SUBVCFNAME}.vcf.gz \
  --out ${SUBVCFNAME} \
  --depth 
  
~/Software/vcftools_jydu/src/cpp/vcftools \
  --gzvcf ${SUBVCFNAME}.vcf.gz  \
  --out ${SUBVCFNAME} \
  --missing-indv  
```


```{bash, eval = F}
gunzip ${SUBVCFNAME}.vcf.gz
~/Software/plink \
  --distance square ibs \
  --vcf ${SUBVCFNAME}.vcf \
  --out ${SUBVCFNAME} \
  --const-fid
```

First, let's look at missing data and depth. Samples with a large amount of missing data are increasing the amount of positions with missing data (which will be filtered out in some instances) for nothing. So I would rather remove some samples completely rather than a higher number of positions for all samples.
```{r}
info_per_ind = full_join(read_tsv(paste0(subset_vcf_name, ".imiss")), 
                         read_tsv(paste0(subset_vcf_name, ".idepth"))) %>%
  filter(!(INDV %in% unwanted_samples))

NA_thr = 0.2
p = ggplot(info_per_ind, aes(x = F_MISS, y = MEAN_DEPTH)) + 
  geom_point(alpha = 0.6) +
  geom_vline(aes(xintercept = NA_thr), col = "#ffa62b") + 
  theme_classic() +
  labs(title = "Missing data and depth in vcf",
       subtitle = str_wrap(paste0("There are ", sum(info_per_ind$F_MISS > NA_thr), 
                         " samples with too much missing data and ", 
                         sum(info_per_ind$F_MISS <= NA_thr), " that are fine."), 
                         width = 60))
p1 <- ggMarginal(p, type="histogram", size = 2)
p1

samples_to_filter = info_per_ind %>% 
  filter(F_MISS > NA_thr) %>%
  select(INDV) %>% 
  pull()

info_per_ind %>% 
  filter(F_MISS > NA_thr) %>%
  select(INDV) %>%
  write_tsv(paste0(data_dir, "Sample_with_too_much_NA.args"), col_names = F)
```


The samples with too much missing data, on the right of the gold line, will be filtered out.

```{r}
ids = read_tsv(paste0(subset_vcf_name, ".mibs.id"), col_names = c("Whatever", "ID")) %>% select(ID) %>% pull()
related = read_tsv(paste0(subset_vcf_name, ".mibs"), col_names = ids) %>%
  mutate(ID1 = ids) %>%
  pivot_longer(names_to = "ID2", values_to = "Relatedness", cols = -ID1) %>%
  filter(!(ID1 %in% unwanted_samples)) %>%
  filter(!(ID2 %in% unwanted_samples))

related %>%
  filter(ID1 != ID2) %>%
  ggplot(aes(x = Relatedness)) +
  geom_density() + 
  geom_vline(aes(xintercept = 0.9))

related = related %>% mutate(Bounded_relatedness = ifelse(Relatedness < 0.9, 0.9, Relatedness))

related %>%
  filter(ID1 != ID2) %>%
  ggplot(aes(x = Bounded_relatedness)) +
  geom_density() + 
  geom_vline(aes(xintercept = 0.9)) + 
  geom_vline(aes(xintercept = 0.99))

ggplot(related, aes(x = ID1, y = ID2, fill = Bounded_relatedness)) +
  geom_tile() +
  scale_fill_gradient(high = "white", low = "#16697a") +
  labs(title = "Pairwise IBS across all samples")
```

Now, I want to represent the same measure but including only the sample which have an IBS of at least 0.99 with any other sample.
```{r}

temp = related %>% 
  filter(Relatedness >= 0.99) %>%
  filter(ID1 != ID2) %>%
  filter(!(ID1 %in% samples_to_filter)) %>%
  filter(!(ID2 %in% samples_to_filter))
vector_temp = c(temp %>% select(ID1) %>% pull(),
        temp %>% select(ID2) %>% pull())
table_temp = as.data.frame(table(vector_temp))
length(unique(vector_temp))

related %>% 
  filter(!(ID1 %in% samples_to_filter)) %>%
  filter(!(ID2 %in% samples_to_filter)) %>%
  filter(ID1 %in% vector_temp) %>%
  filter(ID2 %in% vector_temp) %>%
  ggplot(aes(x = ID1, y = ID2, fill = Relatedness)) +
  geom_tile() +
  scale_fill_gradient(high = "white", low = "#16697a")

```

```{r}

#for the representation, I first filter the samples to remove all samples with only one connection 
# (so they appear twice in the table)
nodes <- table_temp %>% 
  filter(Freq > 2) %>%
  select(label = vector_temp) %>% 
  rowid_to_column("id") 

#Here I'm renaming the samples with the node number and filtering
edges <- temp %>% 
  filter(ID1 %in% nodes$label) %>%
  filter(ID2 %in% nodes$label) %>%
  select(-Bounded_relatedness) %>%
  left_join(nodes, by = c("ID1" = "label")) %>% 
  rename(from = id)

edges <- edges %>% 
  left_join(nodes, by = c("ID2" = "label")) %>% 
  rename(to = id) %>%
  select(from, to, weight = Relatedness )

#I can then transform the data in a tbl_graph
#I also want to color the nodes based on the component

routes_tidy <- tbl_graph(nodes = nodes, edges = edges, directed = F)

ggraph(routes_tidy) + 
  geom_edge_link(color = "gray20", alpha = 0.3) + 
  geom_node_point(aes(color = as.character(group_components())),show.legend = F) +
  theme_graph() #+ facet_nodes(~ group_components())
```

This graphs shows that the components (subgraphs?) are accurately identified, as shown by the colors. This is the information I need since I would like to filter keeping only one sample per group. Now, I want to do this for all samples, not just the ones with more than one edge.
```{r}
nodes <- table_temp %>%
  select(label = vector_temp) %>% 
  rowid_to_column("id") 

#Here I'm renaming the samples with the node number and filtering
edges <- temp %>% 
  filter(ID1 %in% nodes$label) %>%
  filter(ID2 %in% nodes$label) %>%
  select(-Bounded_relatedness) %>%
  left_join(nodes, by = c("ID1" = "label")) %>% 
  rename(from = id)

edges <- edges %>% 
  left_join(nodes, by = c("ID2" = "label")) %>% 
  rename(to = id) %>%
  select(from, to, weight = Relatedness )

#I can then transform the data in a tbl_graph
#I also want to color the nodes based on the component

routes_tidy <- tbl_graph(nodes = nodes, edges = edges, directed = F)
#Once I know it works as I expect, I get the group information for all samples !
routes_tidy <- tbl_graph(nodes = nodes, edges = edges, directed = F) %>%
  mutate(group = group_components())

#I extract the group/component information 
# and merge this with the depth and NA data per sample
nodes_with_groups <- routes_tidy %>%
  activate(nodes) %>%
  data.frame() %>%
  left_join(., info_per_ind, by = c("label" = "INDV"))

#Identifying the sample with the lowest amount of missing data per group
nodes_with_groups = nodes_with_groups %>%
  group_by(group) %>%
  mutate(rank  = rank(F_MISS, ties.method = "random")) %>%
  mutate(Filter_based_on_ibs = ifelse(rank == 1, "Keep", "Filter_out"))

#Writing a file to indicate which samples to remove
nodes_with_groups %>%
  filter(Filter_based_on_ibs == "Filter_out") %>%
  ungroup() %>%
  select(label) %>%
  write_tsv(paste0(data_dir, "Sample_removed_based_on_IBS.args"), col_names = F)

#Let's make a pretty doughnut plot just for the fun of it.
as.data.frame(table(nodes_with_groups$Filter_based_on_ibs)) %>%
  ggplot(aes(x=2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity") +
  xlim(0.5, 2.5) +
  coord_polar(theta = "y") +
  scale_fill_manual(values = c("#16697a", "#ffa62b")) +
  labs(x=NULL, y=NULL, fill="",
       title = "Filtering based on IBS threshold and depth/NA",
       subtitle = "For each group, the sample with the lowest amount of missing data is kept.") +
  theme_bw() +
  theme(plot.title = element_text(face="bold",family=c("sans"),size=15),
        legend.text=element_text(size=10),
        axis.ticks=element_blank(),
        axis.text=element_blank(),
        axis.title=element_blank(),
        panel.grid=element_blank(),
        panel.border=element_blank()) + 
  geom_text(aes(label = Freq), 
               position = position_stack(vjust = 0.5), size = 4)

```




