---
title: "SNP filtering"
author: "Alice Feurtey"
date: "01/13/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=TRUE}
#Libraries needed
library(tidyverse)
library(ggExtra)
library(tidygraph)
library(ggraph)
library(cowplot)

#Chromosome choice for different steps
test_CHR = 10
subset_CHR = 1

#Files and directories 
dir_name = "/data2/alice/WW_project/"
data_dir = "/data2/alice/WW_project/0_Data/"
test_vcf_name = paste0(dir_name, 
                  "1_Variant_calling/4_Joint_calling/",
                  "Ztritici_global_December2020.genotyped.", 
                  test_CHR)
subset_vcf_name = paste0(dir_name, 
                  "1_Variant_calling/4_Joint_calling/",
                  "Ztritici_global_December2020.genotyped.", 
                  subset_CHR,
                  ".filtered.clean")
ref_genome = paste0(dir_name, 
                    "0_Data/",
                    "Zymoseptoria_tritici.MG2.dna.toplevel.mt+.fa")
vcf_qual_check_dir =paste0(dir_name, 
                           "1_Variant_calling/5_Quality_checks/")

soft_dir = "/home/alice/Software/"
list_dir = "/home/alice/WW_PopGen/Keep_lists_samples/"
gatk_path = paste0(soft_dir, "gatk-4.1.4.1/gatk")
#java_path = "/Library/Java/JavaAppletPlugin.plugin/Contents/Home/bin/java"
bcftools_path = paste0(soft_dir, "bcftools-1.10.2/")

#This is a file that contains the name of all samples I am not interested in
# for example Michael Habig's mutants and other resequencing of IPO323.
unwanted_samples=read_tsv(paste0(data_dir, "Samples_to_filter_out.args"), col_names = F) %>% pull()

Sys.setenv(DIRNAME=dir_name)
Sys.setenv(TESTVCFNAME=test_vcf_name)
Sys.setenv(SUBVCFNAME=subset_vcf_name)
Sys.setenv(REF=ref_genome)
Sys.setenv(GATKPATH=gatk_path)
#Sys.setenv(JAVAPATH=java_path)
Sys.setenv(BCFTOOLSPATH=bcftools_path)

```


# Filtering based on site: identifying the proper values
Below I first extract the snps and in a second time, I remove all genotypes with a depth that I consider too low for the data to be reliable. This threshold can and should be adapted depending on the dataset.
Finally, I use bcftools to extract the values of the different estimates we can filter on.
```{bash, eval = F}

#Removing indels
${GATKPATH} SelectVariants  \
  -R ${REF} \
  -V ${TESTVCFNAME}.vcf.gz \
 --select-type-to-include SNP \
 -O ${TESTVCFNAME}.snps.vcf

#Filtering on low depth per genotype
${GATKPATH} VariantFiltration \
 -R ${REF} \
 -V ${TESTVCFNAME}.snps.vcf \
--genotype-filter-expression "DP < 3"  \
--set-filtered-genotype-to-no-call   \
--genotype-filter-name "Low_depth"   \
-O ${TESTVCFNAME}.snps.low_depth.vcf
```
Here, I clean the file to remove alternate alleles that do not exist anymore because they were only found in the filtered genotyped. And I additionally remove the positions which are non-variant, now that some genotypes are filtered out.

```{bash, eval = F}
${GATKPATH} SelectVariants  \
  -R ${REF} \
  -V ${TESTVCFNAME}.snps.low_depth.vcf \
 --exclude-non-variants --remove-unused-alternates \
 -O ${TESTVCFNAME}.snps.low_depth.variants.vcf
 
```

```{bash, eval = F}
#Extracting the data for the figures
${BCFTOOLSPATH}bcftools query -f '%CHROM %POS %MQ %QD %FS %DP %MQRankSum %QUAL %ReadPosRankSum %BaseQRankSum\n'  ${TESTVCFNAME}.snps.low_depth.variants.vcf > ${TESTVCFNAME}.snps.low_depth.variants.info_fields.tab

```

Now that we have the data extracted, I can make figures and test values for the filtering.

```{r}
FS_thr = 10
MQ_thr = 20
QD_thr = 20
DP_thr = 60000
ReadPosRankSum_thr_high = 2
ReadPosRankSum_thr_low = -2
MQRankSum_thr_high = 2
MQRankSum_thr_low = -2
BaseQRankSum_thr_high = 2
BaseQRankSum_thr_low = -2
  
print(paste0(test_vcf_name, ".snps.low_depth.variants.info_fields.tab"))
info = read_delim(paste0(test_vcf_name, ".snps.low_depth.variants.info_fields.tab"),
                  delim = " ", col_names =  c("CHROM", "POS", "MQ", "QD", "FS", "DP",
                                              "MQRankSum", "QUAL", "ReadPosRankSum", "BaseQRankSum"), na = ".")
temp = info %>% 
  gather(key = "info_field", value = "value", -CHROM, -POS) %>% 
  mutate (Filter = "Unfiltered")

temp2 = info  %>% filter(FS < FS_thr & MQ > MQ_thr & QD > QD_thr & DP < DP_thr) %>% 
  filter(between(MQRankSum, MQRankSum_thr_low, MQRankSum_thr_high) | is.na(MQRankSum)) %>% 
  filter(between(BaseQRankSum, BaseQRankSum_thr_low, BaseQRankSum_thr_high) | is.na(BaseQRankSum)) %>% 
  filter(between(ReadPosRankSum, ReadPosRankSum_thr_low, ReadPosRankSum_thr_high) | is.na(ReadPosRankSum)) %>% 
  filter(QUAL > 500) %>%
  gather(key = "info_field", value = "value", -CHROM, -POS) %>% 
  mutate (Filter = "Filtered")

p = ggplot(temp, aes(value, fill = Filter)) + geom_density(alpha=0.6) 
p + facet_wrap(info_field~., scales="free")  + theme_classic() 

p2 = ggplot(temp2, aes(value, fill = Filter)) + geom_density(alpha=0.6) 
p2 + facet_wrap(info_field~., scales="free")  + theme_classic() 


dim(temp %>% select(CHROM, POS) %>% group_by_all %>% count)
dim(temp2 %>% select(CHROM, POS) %>% group_by_all %>% count) 


Sys.setenv(FSTHR = sprintf("%0.2f", FS_thr))
Sys.setenv(MQTHR = sprintf("%0.2f", MQ_thr))
Sys.setenv(QDTHR = sprintf("%0.2f", QD_thr))
Sys.setenv(DPTHR = sprintf("%0.2f", DP_thr))
```

The values obtained above can be used in a script to run directly on the cluster for all chromosomes.



# Filter checks using resequencing

I want to assess the effect of the filtering on the SNP data. 
```{r Get variants from filter}

var_list = bind_rows(paste0(vcf_qual_check_dir, "Ztritici_global_December2020.genotyped.", c(1:21), ".tab") %>%
                         map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT"), 
                           col_types = cols(col_character(), col_double(), col_character(), col_character()))) %>%
                         mutate(Filter = "Not_filtered"),
                     paste0(vcf_qual_check_dir, "Ztritici_global_December2020.genotyped.", c(1:21), ".filtered.clean.tab") %>%
                         map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT"), 
                           col_types = cols(col_character(), col_double(), col_character(), col_character()))) %>%
                         mutate(Filter = "Filtered_on_position")) %>% 
  mutate(Nb_allele = str_count(ALT, pattern = ",") + 2) %>%
  mutate(Nb_letter = str_count(ALT, pattern = "[A-Z]") + str_count(REF, pattern = "[A-Z]")) %>%
  mutate(Indel = ifelse((str_count(ALT, pattern = "[*]") + str_count(REF, pattern = "[*]")) > 0, "Indel", 
                        ifelse(Nb_letter != Nb_allele, "Indel", "SNP"))) %>%
  unite(col = "Var_type", Indel, Nb_allele, sep = "_", remove = F)

var_list  %>%
group_by(CHROM, Filter, Var_type) %>%
count()  %>%
ggplot(aes(x=CHROM, y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "stack") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_wrap(vars(Filter)) +
  coord_flip() +
  labs(title = "Variants number before and after filtering on position quality")

```


We have 9 isolates that were resequenced several times in different datasets. Using these, we can estimate the amount of false positive that happen and the type of variants that are more susceptible to be affected by such errors.
```{r tab per pair}
reseq_list = read_delim(paste0(list_dir, "Repeat_sequencing.txt"), delim =" ", col_names = c("Name_1", "Name_2"))

list_diff = list()

for (sample in c(1:length(reseq_list$Name_1))) {

#sample = 1

list_filtered = list.files(path = vcf_qual_check_dir, pattern = paste0("filtered.clean.", reseq_list$Name_1[sample], ".tab"), full.names = T) 
list_unfiltered = setdiff(list.files(path = vcf_qual_check_dir, pattern = paste0(reseq_list$Name_1[sample], ".tab"), full.names = T),
                     list.files(path = vcf_qual_check_dir, pattern = paste0("filtered.clean.", reseq_list$Name_1[sample], ".tab"), 
                                full.names = T)) 

filtered = list_filtered %>% 
    map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT", "Clone1", "Clone2"), 
                        col_types = cols(col_character(), col_double(), col_character(), col_character(), col_character(), col_character()))) %>%
    mutate(Filter = "Filtered_on_position") 



unfiltered = list_unfiltered %>% 
    map_df(~read_tsv(., col_names = c("CHROM", "POS", "REF", "ALT", "Clone1", "Clone2"), 
                        col_types = cols(col_character(), col_double(), col_character(), col_character(), col_character(), col_character()))) %>%
    mutate(Filter = "Not_filtered") 


head(unfiltered)
list_diff[[sample]] = bind_rows(filtered, unfiltered) %>% mutate(Sample = reseq_list$Name_1[sample]) 
}

temp2 = bind_rows(list_diff) %>% 
  mutate(Nb_allele = str_count(ALT, pattern = ",") + 2) %>%
  mutate(Nb_letter = str_count(ALT, pattern = "[A-Z]") + str_count(REF, pattern = "[A-Z]")) %>%
  mutate(Indel = ifelse((str_count(ALT, pattern = "[*]") + str_count(REF, pattern = "[*]")) > 0, "Indel", 
                        ifelse(Nb_letter != Nb_allele, "Indel", "SNP"))) %>%
  unite(col = "Var_type", Indel, Nb_allele, sep = "_", remove = F)

temp2  %>%
group_by(Filter, Sample, Var_type) %>%
count()  %>%
ggplot(aes(x=Sample, y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "stack") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_wrap(vars(Filter)) +
  coord_flip()

temp2 %>%
group_by(CHROM, Filter, Sample, Var_type) %>%
count()  %>%
ggplot(aes(x=as.numeric(CHROM), y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "stack") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_grid(cols = vars(Filter), rows = vars(Sample)) 

temp2  %>%
  select(CHROM, POS, Filter, Var_type) %>%
  distinct() %>%
group_by(CHROM, Filter, Var_type) %>%
count()  %>%
ggplot(aes(x=as.numeric(CHROM), y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "stack") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_grid(cols = vars(Filter)) 
```


Let's visualize both datasets togethers so we can compare the proportions of all variants vs errors.
```{r}
#Bar plots of the comparison
bind_rows(temp2 %>%
            unite(col = "Var_type", Indel, Nb_allele, sep = "_")%>%
            select(CHROM, POS, Filter, Var_type) %>%
            distinct() %>% 
            mutate(Dataset = "Clones"),
          var_list%>% 
            mutate(Dataset = "All_variants")) %>%
  group_by(Filter, Var_type, Dataset) %>%
count()  %>%
ggplot(aes(x=Dataset, y =n, fill = Var_type)) +
 geom_bar(stat = "identity", position = "fill") +
 theme_bw() +
  scale_fill_viridis_d() +
  facet_grid(cols = vars(Filter))


#Let's make a pretty doughnut plot just for the fun of it.
t1 = temp2 %>%
  select(CHROM, POS, Filter, Var_type) %>%
  distinct() %>% 
  group_by(Filter) %>%
  count()  


p1 = tibble(Filter_status = c("Filtered out", "Kept after filtering"), 
       Nb_variants = c((t1[2, 2] - t1[1, 2]) %>% pull(), t1[1, 2] %>% pull())) %>%
  ggplot(aes(x=2, y=Nb_variants, fill=Filter_status)) +
  geom_bar(stat="identity") +
  xlim(0.5, 2.5) +
  coord_polar(theta = "y") +
  #scale_fill_manual(values = c("#16697a", "#ffa62b")) +
  labs(x=NULL, y=NULL, fill="",
       title = "Errors from resequencing") +
  theme_bw() +
  theme(plot.title = element_text(face="bold",family=c("sans"),size=15),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        axis.ticks=element_blank(),
        axis.text=element_blank(),
        axis.title=element_blank(),
        panel.grid=element_blank(),
        panel.border=element_blank())+ 
  geom_text(aes(label = Nb_variants), 
               position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_viridis_d()

t1 = var_list %>% 
  group_by(Filter) %>%
  count()   

p2 = tibble(Filter_status = c("Filtered out", "Kept after filtering"), 
       Nb_variants = c((t1[2, 2] - t1[1, 2]) %>% pull(), t1[1, 2] %>% pull())) %>%
  ggplot(aes(x=2, y=Nb_variants, fill=Filter_status)) +
  geom_bar(stat="identity") +
  xlim(0.5, 2.5) +
  coord_polar(theta = "y") +
  #scale_fill_manual(values = c("#16697a", "#ffa62b")) +
  labs(x=NULL, y=NULL, fill="",
       title = "General dataset") +
  theme_bw() +
  theme(plot.title = element_text(face="bold",family=c("sans"),size=15),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        axis.ticks=element_blank(),
        axis.text=element_blank(),
        axis.title=element_blank(),
        panel.grid=element_blank(),
        panel.border=element_blank()) + 
  geom_text(aes(label = Nb_variants), 
               position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_viridis_d()


t1 = temp2 %>%
  filter(Indel == "SNP") %>%
  select(CHROM, POS, Filter, Var_type) %>%
  distinct() %>% 
  group_by(Filter) %>%
  count()  


p3 = tibble(Filter_status = c("Filtered out", "Kept after filtering"), 
       Nb_variants = c((t1[2, 2] - t1[1, 2]) %>% pull(), t1[1, 2] %>% pull())) %>%
  ggplot(aes(x=2, y=Nb_variants, fill=Filter_status)) +
  geom_bar(stat="identity") +
  xlim(0.5, 2.5) +
  coord_polar(theta = "y") +
  #scale_fill_manual(values = c("#16697a", "#ffa62b")) +
  labs(x=NULL, y=NULL, fill="",
       title = "Errors from resequencing: SNPs") +
  theme_bw() +
  theme(plot.title = element_text(face="bold",family=c("sans"),size=15),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        axis.ticks=element_blank(),
        axis.text=element_blank(),
        axis.title=element_blank(),
        panel.grid=element_blank(),
        panel.border=element_blank())+ 
  geom_text(aes(label = Nb_variants), 
               position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_viridis_d()

t1 = var_list %>% 
  filter(Indel == "SNP") %>%
  group_by(Filter) %>%
  count()   

p4 = tibble(Filter_status = c("Filtered out", "Kept after filtering"), 
       Nb_variants = c((t1[2, 2] - t1[1, 2]) %>% pull(), t1[1, 2] %>% pull())) %>%
  ggplot(aes(x=2, y=Nb_variants, fill=Filter_status)) +
  geom_bar(stat="identity") +
  xlim(0.5, 2.5) +
  coord_polar(theta = "y") +
  #scale_fill_manual(values = c("#16697a", "#ffa62b")) +
  labs(x=NULL, y=NULL, fill="",
       title = "General dataset: SNPs") +
  theme_bw() +
  theme(plot.title = element_text(face="bold",family=c("sans"),size=15),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        axis.ticks=element_blank(),
        axis.text=element_blank(),
        axis.title=element_blank(),
        panel.grid=element_blank(),
        panel.border=element_blank()) + 
  geom_text(aes(label = Nb_variants), 
               position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_viridis_d()

cowplot::plot_grid(p1, p2, p3, p4, rows =2 )

```

Once we have a general idea of the error quantity, I want to look at the variants distribution along the genomes. Are errors found in the same area of the genomes for all resequencing pairs? 
```{r diff per windows}
chromosomes = read_tsv(paste0(ref_genome, ".fai"),
                        col_names = c("CHROM", "Length", "Byte_index", "Base_per_line", "Bytes_per_line")) 
win_size = 10000

list_temp = list()
for (chr in c(1:length(chromosomes$CHROM))) {
  list_temp[[chr]] = bind_rows(tibble(CHROM = as.character(chr), 
                            Window = seq(from = 0, to = chromosomes$Length[14], by = win_size), 
                            Filter = "Not_filtered"),
                            tibble(CHROM = as.character(chr), 
                            Window = seq(from = 0, to = chromosomes$Length[14], by = win_size), 
                            Filter = "Filtered_on_position"))

}

windows = (bind_rows(list_temp))

temp = bind_rows(list_diff) %>% 
  mutate(Window = win_size*round(POS / win_size)) %>%
  group_by(CHROM, Filter, Sample, Window) %>%
  count() %>%
  pivot_wider(names_from = Sample, values_from = n) %>%
  full_join(., windows) %>%
  pivot_longer(-c(CHROM, Filter, Window), names_to = "Sample", values_to = "Nb_SNP") %>%
  mutate(Nb_SNP = replace_na(Nb_SNP, 0))
  
temp %>%
  filter(Filter == "Not_filtered") %>%
  filter(as.integer(CHROM) <= 6) %>% 
ggplot(., aes(x = Window, y = Nb_SNP, col = Sample)) +
  geom_line(alpha = .4) +
  geom_point(alpha = .4) + 
  theme_minimal() + 
  facet_grid(rows = vars(as.integer(CHROM)), cols = vars(Filter))

temp %>%
  filter(as.integer(CHROM) <= 6) %>% 
ggplot(., aes(x = Window, y = Nb_SNP, col = Sample)) +
  geom_line(alpha = .4) +
  theme_minimal() + 
  facet_grid(rows = vars(as.integer(CHROM)), cols = vars(Filter)) +
  theme(legend.position = "none")


```




# Filtering based on individuals
```{bash, eval = F}
~/Software/vcftools_jydu/src/cpp/vcftools \
  --gzvcf ${SUBVCFNAME}.vcf.gz \
  --out ${SUBVCFNAME} \
  --depth 
  
~/Software/vcftools_jydu/src/cpp/vcftools \
  --gzvcf ${SUBVCFNAME}.vcf.gz  \
  --out ${SUBVCFNAME} \
  --missing-indv  
```


```{bash, eval = F}
gunzip ${SUBVCFNAME}.vcf.gz
~/Software/plink \
  --distance square ibs \
  --vcf ${SUBVCFNAME}.vcf \
  --out ${SUBVCFNAME} \
  --const-fid
```

First, let's look at missing data and depth. Samples with a large amount of missing data are increasing the amount of positions with missing data (which will be filtered out in some instances) for nothing. So I would rather remove some samples completely rather than a higher number of positions for all samples.
```{r}
info_per_ind = full_join(read_tsv(paste0(subset_vcf_name, ".imiss")), 
                         read_tsv(paste0(subset_vcf_name, ".idepth"))) %>%
  filter(!(INDV %in% unwanted_samples))

NA_thr = 0.2
p = ggplot(info_per_ind, aes(x = F_MISS, y = MEAN_DEPTH)) + 
  geom_point(alpha = 0.6) +
  geom_vline(aes(xintercept = NA_thr), col = "#ffa62b") + 
  theme_classic() +
  labs(title = "Missing data and depth in vcf",
       subtitle = str_wrap(paste0("There are ", sum(info_per_ind$F_MISS > NA_thr), 
                         " samples with too much missing data and ", 
                         sum(info_per_ind$F_MISS <= NA_thr), " that are fine."), 
                         width = 60))
p1 <- ggMarginal(p, type="histogram", size = 2)
p1

samples_to_filter = info_per_ind %>% 
  filter(F_MISS > NA_thr) %>%
  select(INDV) %>% 
  pull()

info_per_ind %>% 
  filter(F_MISS > NA_thr) %>%
  select(INDV) %>%
  write_tsv(paste0(data_dir, "Sample_with_too_much_NA.args"), col_names = F)
```
The samples with too much missing data, on the right of the gold line, will be filtered out.

```{r}
ids = read_tsv(paste0(subset_vcf_name, ".mibs.id"), col_names = c("Whatever", "ID")) %>% select(ID) %>% pull()
related = read_tsv(paste0(subset_vcf_name, ".mibs"), col_names = ids) %>%
  mutate(ID1 = ids) %>%
  pivot_longer(names_to = "ID2", values_to = "Relatedness", cols = -ID1) %>%
  filter(!(ID1 %in% unwanted_samples)) %>%
  filter(!(ID2 %in% unwanted_samples))

related %>%
  filter(ID1 != ID2) %>%
  ggplot(aes(x = Relatedness)) +
  geom_density() + 
  geom_vline(aes(xintercept = 0.9))

related = related %>% mutate(Bounded_relatedness = ifelse(Relatedness < 0.9, 0.9, Relatedness))

related %>%
  filter(ID1 != ID2) %>%
  ggplot(aes(x = Bounded_relatedness)) +
  geom_density() + 
  geom_vline(aes(xintercept = 0.9)) + 
  geom_vline(aes(xintercept = 0.99))

ggplot(related, aes(x = ID1, y = ID2, fill = Bounded_relatedness)) +
  geom_tile() +
  scale_fill_gradient(high = "white", low = "#16697a") +
  labs(title = "Pairwise IBS across all samples")
```

Now, I want to represent the same measure but including only the sample which have an IBS of at least 0.99 with any other sample.
```{r}

temp = related %>% 
  filter(Relatedness >= 0.99) %>%
  filter(ID1 != ID2) %>%
  filter(!(ID1 %in% samples_to_filter)) %>%
  filter(!(ID2 %in% samples_to_filter))
vector_temp = c(temp %>% select(ID1) %>% pull(),
        temp %>% select(ID2) %>% pull())
table_temp = as.data.frame(table(vector_temp))
length(unique(vector_temp))

related %>% 
  filter(!(ID1 %in% samples_to_filter)) %>%
  filter(!(ID2 %in% samples_to_filter)) %>%
  filter(ID1 %in% vector_temp) %>%
  filter(ID2 %in% vector_temp) %>%
  ggplot(aes(x = ID1, y = ID2, fill = Relatedness)) +
  geom_tile() +
  scale_fill_gradient(high = "white", low = "#16697a")

```

```{r}

#for the representation, I first filter the samples to remove all samples with only one connection 
# (so they appear twice in the table)
nodes <- table_temp %>% 
  filter(Freq > 2) %>%
  select(label = vector_temp) %>% 
  rowid_to_column("id") 

#Here I'm renaming the samples with the node number and filtering
edges <- temp %>% 
  filter(ID1 %in% nodes$label) %>%
  filter(ID2 %in% nodes$label) %>%
  select(-Bounded_relatedness) %>%
  left_join(nodes, by = c("ID1" = "label")) %>% 
  rename(from = id)

edges <- edges %>% 
  left_join(nodes, by = c("ID2" = "label")) %>% 
  rename(to = id) %>%
  select(from, to, weight = Relatedness )

#I can then transform the data in a tbl_graph
#I also want to color the nodes based on the component

routes_tidy <- tbl_graph(nodes = nodes, edges = edges, directed = F)

ggraph(routes_tidy) + 
  geom_edge_link(color = "gray20", alpha = 0.3) + 
  geom_node_point(aes(color = as.character(group_components())),show.legend = F) +
  theme_graph() #+ facet_nodes(~ group_components())
```

This graphs shows that the components (subgraphs?) are accurately identified, as shown by the colors. This is the information I need since I would like to filter keeping only one sample per group. Now, I want to do this for all samples, not just the ones with more than one edge.
```{r}
nodes <- table_temp %>%
  select(label = vector_temp) %>% 
  rowid_to_column("id") 

#Here I'm renaming the samples with the node number and filtering
edges <- temp %>% 
  filter(ID1 %in% nodes$label) %>%
  filter(ID2 %in% nodes$label) %>%
  select(-Bounded_relatedness) %>%
  left_join(nodes, by = c("ID1" = "label")) %>% 
  rename(from = id)

edges <- edges %>% 
  left_join(nodes, by = c("ID2" = "label")) %>% 
  rename(to = id) %>%
  select(from, to, weight = Relatedness )

#I can then transform the data in a tbl_graph
#I also want to color the nodes based on the component

routes_tidy <- tbl_graph(nodes = nodes, edges = edges, directed = F)
#Once I know it works as I expect, I get the group information for all samples !
routes_tidy <- tbl_graph(nodes = nodes, edges = edges, directed = F) %>%
  mutate(group = group_components())

#I extract the group/component information 
# and merge this with the depth and NA data per sample
nodes_with_groups <- routes_tidy %>%
  activate(nodes) %>%
  data.frame() %>%
  left_join(., info_per_ind, by = c("label" = "INDV"))

#Identifying the sample with the lowest amount of missing data per group
nodes_with_groups = nodes_with_groups %>%
  group_by(group) %>%
  mutate(rank  = rank(F_MISS, ties.method = "random")) %>%
  mutate(Filter_based_on_ibs = ifelse(rank == 1, "Keep", "Filter_out"))

#Writing a file to indicate which samples to remove
nodes_with_groups %>%
  filter(Filter_based_on_ibs == "Filter_out") %>%
  ungroup() %>%
  select(label) %>%
  write_tsv(paste0(data_dir, "Sample_removed_based_on_IBS.args"), col_names = F)

#Let's make a pretty doughnut plot just for the fun of it.
as.data.frame(table(nodes_with_groups$Filter_based_on_ibs)) %>%
  ggplot(aes(x=2, y=Freq, fill=Var1)) +
  geom_bar(stat="identity") +
  xlim(0.5, 2.5) +
  coord_polar(theta = "y") +
  scale_fill_manual(values = c("#16697a", "#ffa62b")) +
  labs(x=NULL, y=NULL, fill="",
       title = "Filtering based on IBS threshold and depth/NA",
       subtitle = "For each group, the sample with the lowest amount of missing data is kept.") +
  theme_bw() +
  theme(plot.title = element_text(face="bold",family=c("sans"),size=15),
        legend.text=element_text(size=10),
        axis.ticks=element_blank(),
        axis.text=element_blank(),
        axis.title=element_blank(),
        panel.grid=element_blank(),
        panel.border=element_blank()) + 
  geom_text(aes(label = Freq), 
               position = position_stack(vjust = 0.5), size = 4)

```




